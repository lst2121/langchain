{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LANGCHAIN_API_KEY = os.getenv(key=\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(key=\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(key=\"LANGCHAIN_TRACING_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Summarize the following Question based on the context:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(model=\"phi\", temperature=0, timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://blog.langchain.dev/announcing-langsmith\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap = scrape_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "\n",
    "if sys.platform:\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "import json\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure asyncio for Windows\n",
    "if sys.platform:\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDuckGo Search\n",
    "RESULTS_PER_QUESTION = 3\n",
    "ddg_search = DuckDuckGoSearchAPIWrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform web search\n",
    "def web_search(query: str, num_results: int = RESULTS_PER_QUESTION):\n",
    "    results = ddg_search.results(query, num_results)\n",
    "    return [r[\"link\"] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for summarization\n",
    "SUMMARY_TEMPLATE = \"\"\"{text} \n",
    "\n",
    "-----------\n",
    "Using the above text, answer in short the following question: \n",
    "> {question}\n",
    "-----------\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats etc if available.\"\"\"\n",
    "SUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama model for summarization\n",
    "llm = ChatOllama(model=\"phi\", temperature=0, timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape text from a webpage\n",
    "def scrape_text(url: str):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable chain for scraping and summarizing text\n",
    "scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
    "    text=lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | SUMMARY_PROMPT | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://blog.langchain.dev/announcing-langsmith'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://blog.langchain.dev/announcing-langsmith\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LangSmith is a tool that helps developers debug and evaluate their language models by providing an easy-to-use interface to trace and visualize the sequence of calls in a chain or prompt. It also allows for testing and monitoring the performance of the application. LangSmith can be used to improve the efficiency and effectiveness of LLM applications, and is particularly useful for teams that are striving for a more effective approach.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_and_summarize_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is langsmith?\",\n",
    "        \"url\": url\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable chain for web search\n",
    "web_search_chain = RunnablePassthrough.assign(\n",
    "    urls=lambda x: web_search(x[\"question\"])\n",
    ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Gen_AI_Tutorials\\langchain\\venv\\lib\\site-packages\\curl_cffi\\aio.py:204: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  warnings.warn(\"Curlm alread closed! quitting from process_data\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' LangSmith is a tool that helps developers debug and evaluate their language models by providing an easy-to-use interface for tracing and evaluating complex agent prompt chains. It allows users to create datasets of examples they care about and run any changed prompts over those data sets. LangSmith also provides tools for monitoring the performance of applications, including system-level performance, model/chain performance, and user interactions.\\n',\n",
       " ' LangSmith is a tool that allows users to evaluate their language models and chatbots by providing them with prompts and evaluating their responses. It provides a platform for users to test their models in real-time and get feedback on their performance. LangSmith also offers tools for analyzing the output of the model, such as parsing and sentiment analysis.\\n',\n",
       " ' LangSmith is a tool that helps developers build and deploy their own language models for various applications. It provides an extensible platform with features like code generation, deployment, and monitoring of LLMs. The author believes that there could be significant impact if the core of LangSmith can be built into other applications and services. However, it remains to be seen whether LangSmith can build a long-term defensible business.\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is langsmith?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for search queries\n",
    "SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Write 3 Google search queries to search online that form an objective opinion from the following: {question}\\n\"\n",
    "            \"You must respond with a list of strings in the following format: ['query 1', 'query 2', 'query 3'].\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama model for handling search prompts\n",
    "gemma_llm = ChatOllama(model=\"gemma:2b\", temperature=0, timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable chain for processing search prompts\n",
    "search_question_chain = SEARCH_PROMPT | gemma_llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocation of the search_question_chain\n",
    "res = search_question_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is the difference between langchain and langsmith?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Langchain vs Langsmith: Understanding the Differences\n",
      "2. Exploring the Landscape of Langchains and Langsmiths\n",
      "3. Insights into Langchain and Langsmith: Key Differences and Similarities\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = res.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['1. Langchain vs Langsmith: Understanding the Differences', '2. Exploring the Landscape of Langchains and Langsmiths', '3. Insights into Langchain and Langsmith: Key Differences and Similarities']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(queries))\n",
    "print(queries)\n",
    "print(type(queries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Exploring the Landscape of Langchains and Langsmiths\n"
     ]
    }
   ],
   "source": [
    "print(queries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_function = lambda x: [{\"question\": q} for q in x]\n",
    "output = output_function(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': '1. Langchain vs Langsmith: Understanding the Differences'}, {'question': '2. Exploring the Landscape of Langchains and Langsmiths'}, {'question': '3. Insights into Langchain and Langsmith: A Comparative Analysis'}]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define your search_question_chain\n",
    "search_question_chain1 = SEARCH_PROMPT | gemma_llm | StrOutputParser() | (lambda x: x.splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocation of the search_question_chain\n",
    "res1 = search_question_chain1.invoke(\n",
    "    {\n",
    "        \"question\": \"what is the difference between langchain and langsmith?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Langchain vs Langsmith: Understanding the Differences', '2. Exploring the Landscape of Langchains and Langsmiths', '3. Insights into Langchain and Langsmith: Key Differences and Similarities']\n"
     ]
    }
   ],
   "source": [
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_question_chain2 = (\n",
    "    SEARCH_PROMPT\n",
    "    | gemma_llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: [{\"question\": q} for q in x.splitlines()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = search_question_chain2.invoke(\n",
    "    {\n",
    "        \"question\": \"what is the difference between langchain and langsmith?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': '1. Langchain vs Langsmith: Understanding the Differences'}, {'question': '2. Exploring the Landscape of Langchains and Langsmiths'}, {'question': '3. Insights into Langchain and Langsmith: A Comparative Analysis'}]\n"
     ]
    }
   ],
   "source": [
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Langchain vs Langsmith: Understanding the Differences', '2. Exploring the Landscape of Langchains and Langsmiths', '3. Insights into Langchain and Langsmith: A Comparative Analysis']\n"
     ]
    }
   ],
   "source": [
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_function1 = lambda x: [{\"question\": q} for q in x]\n",
    "output = output_function1(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': '1. Langchain vs Langsmith: Understanding the Differences'}, {'question': '2. Exploring the Landscape of Langchains and Langsmiths'}, {'question': '3. Insights into Langchain and Langsmith: Key Differences and Similarities'}]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = search_question_chain1 | (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()\n",
    "chain = RunnablePassthrough.assign(\n",
    "    queries = search_question_chain1\n",
    ") | (lambda x: [{\"question\": q} for q in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable chain for web search\n",
    "web_search_chain = RunnablePassthrough.assign(\n",
    "    urls=lambda x: web_search(x[\"question\"])\n",
    ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'question'}, {'question': 'queries'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what is the difference between langchain and huggingface?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
