{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline as QP,\n",
    "    Link,\n",
    "    InputComponent,\n",
    ")\n",
    "from llama_index.core.query_engine.pandas import PandasInstructionParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:phoenix.datasets.dataset:Dataset: phoenix_dataset_a8c673da-e85b-49ab-8f4a-7351432662f1 initialized\n",
      "Dataset: phoenix_dataset_a8c673da-e85b-49ab-8f4a-7351432662f1 initialized\n",
      "No active session to close\n",
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n",
      "<phoenix.session.session.ThreadSession object at 0x00000251DA217F40>\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "px.close_app()\n",
    "px.launch_app()\n",
    "import llama_index.core\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")\n",
    "print(px.active_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"D:\\Gen_AI_Tutorials\\langchain\\llamaindexrag\\csv_data\\titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_str = (\n",
    "    \"1. Convert the query to Python code using Pandas.\\n\"\n",
    "    # \"2. The final line of code should be a Python expression. \\n\"\n",
    "    \"2. The code should represent a solution to the query.\\n\"\n",
    "    \"3. PRINT ONLY THE EXPRESSION.\\n\"\n",
    "    \"4. Do not quote the expression.\\n\"\n",
    ")\n",
    "\n",
    "pandas_prompt_str = (\n",
    "    \"You are working with a pandas dataframe in Python.\\n\"\n",
    "    \"The name of the dataframe is `df`.\\n\"\n",
    "    \"This is the result of `df.head()`:\\n\"\n",
    "    \"{df_str}\\n\\n\"\n",
    "    \"Follow these instructions:\\n\"\n",
    "    \"{instruction_str}\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Expression:\"\n",
    ")\n",
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n",
    "    \"Pandas Output: {pandas_output}\\n\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "\n",
    "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
    "    instruction_str=instruction_str, df_str=df.head(5)\n",
    ")\n",
    "pandas_output_parser = PandasInstructionParser(df)\n",
    "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction_str = (\n",
    "#     \"1. Convert the query to executable Python code using Pandas.\\n\"\n",
    "#     \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"\n",
    "#     \"3. The code should represent a solution to the query.\\n\"\n",
    "#     \"4. PRINT ONLY THE EXPRESSION.\\n\"\n",
    "#     \"5. Do not quote the expression.\\n\"\n",
    "# )\n",
    "\n",
    "# pandas_prompt_str = (\n",
    "#     \"You are working with a pandas dataframe in Python.\\n\"\n",
    "#     \"The name of the dataframe is `df`.\\n\"\n",
    "#     \"This is the result of `df.head()`:\\n\"\n",
    "#     \"{df_str}\\n\\n\"\n",
    "#     \"Follow these instructions:\\n\"\n",
    "#     \"{instruction_str}\\n\"\n",
    "#     \"Query: {query_str}\\n\\n\"\n",
    "#     \"Expression:\"\n",
    "# )\n",
    "# response_synthesis_prompt_str = (\n",
    "#     \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "#     \"Query: {query_str}\\n\\n\"\n",
    "#     \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n",
    "#     \"Pandas Output: {pandas_output}\\n\\n\"\n",
    "#     \"Response: \"\n",
    "# )\n",
    "\n",
    "# pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
    "#     instruction_str=instruction_str, df_str=df.head(5)\n",
    "# )\n",
    "# pandas_output_parser = PandasInstructionParser(df)\n",
    "# response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= Ollama(model=\"phi\",request_timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp = QP(\n",
    "    modules={\n",
    "        \"input\": InputComponent(),\n",
    "        \"pandas_prompt\": pandas_prompt,\n",
    "        \"llm1\": llm,\n",
    "        \"pandas_output_parser\": pandas_output_parser,\n",
    "        \"response_synthesis_prompt\": response_synthesis_prompt,\n",
    "        \"llm2\": llm,\n",
    "    },\n",
    "    verbose=True,\n",
    ")\n",
    "qp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
    "qp.add_links(\n",
    "    [\n",
    "        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
    "        Link(\n",
    "            \"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"\n",
    "        ),\n",
    "        Link(\n",
    "            \"pandas_output_parser\",\n",
    "            \"response_synthesis_prompt\",\n",
    "            dest_key=\"pandas_output\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# add link from response synthesis prompt to llm2\n",
    "qp.add_link(\"response_synthesis_prompt\", \"llm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'phi',\n",
       " 'created_at': '2024-04-06T19:34:41.6010648Z',\n",
       " 'message': {'role': 'assistant', 'content': ''},\n",
       " 'done': True}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'set_callback_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mqp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the correlation between survival and age?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Gen_AI_Tutorials\\langchain\\venv\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:309\u001b[0m, in \u001b[0;36mQueryPipeline.run\u001b[1;34m(self, return_values_direct, callback_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# first set callback manager\u001b[39;00m\n\u001b[0;32m    308\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_callback_manager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;66;03m# try to get query payload\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Gen_AI_Tutorials\\langchain\\venv\\lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:297\u001b[0m, in \u001b[0;36mQueryPipeline.set_callback_manager\u001b[1;34m(self, callback_manager)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_dict\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 297\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_callback_manager\u001b[49m(callback_manager)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'set_callback_manager'"
     ]
    }
   ],
   "source": [
    "response = qp.run(\n",
    "    query_str=\"What is the correlation between survival and age?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import pandas as pd\n",
    "from llama_index.core.query_engine import PandasQueryEngine\n",
    "\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= Ollama(model=\"phi\",request_timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test on some sample data\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"city\": [\"Toronto\", \"Tokyo\", \"Berlin\"],\n",
    "        \"population\": [2930000, 13960000, 3645000],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = PandasQueryEngine(df=df, verbose=True, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "> Pandas Instructions:\n",
      "```\n",
      " The city with the highest population can be found using the `max()` function and passing it a lambda function that returns the first element of each row in the dataframe, which represents the population value. Then we use the `idxmax()` method to get the index of the row with the maximum population. Finally, we access the city name from this index using the `loc[]` method.\n",
      "\n",
      "```\n",
      "city_with_highest_population = df.iloc[df['population'].apply(lambda x: int(x)).idxmax()]['city']\n",
      "print(city_with_highest_population)\n",
      "```\n",
      "\n",
      "```\n",
      "> Pandas Output: There was an error running the output as Python code. Error message: invalid syntax (<string>, line 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Gen_AI_Tutorials\\langchain\\venv\\lib\\site-packages\\llama_index\\core\\query_engine\\pandas\\output_parser.py\", line 54, in default_output_processor\n",
      "    output_str = str(safe_eval(module_end_str, global_vars, local_vars))\n",
      "  File \"d:\\Gen_AI_Tutorials\\langchain\\venv\\lib\\site-packages\\llama_index\\core\\exec_utils.py\", line 158, in safe_eval\n",
      "    return eval(__source, _get_restricted_globals(__globals), __locals)\n",
      "  File \"<string>\", line 0\n",
      "    \n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is the city with the highest population?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ollama\n",
      "Version: 0.1.8\n",
      "Summary: The official Python client for Ollama.\n",
      "Home-page: https://ollama.ai\n",
      "Author: Ollama\n",
      "Author-email: hello@ollama.com\n",
      "License: MIT\n",
      "Location: d:\\gen_ai_tutorials\\langchain\\venv\\lib\\site-packages\n",
      "Requires: httpx\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "llm = ollama.chat(model='phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'phi',\n",
       " 'created_at': '2024-04-06T19:34:41.6010648Z',\n",
       " 'message': {'role': 'assistant', 'content': ''},\n",
       " 'done': True}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
